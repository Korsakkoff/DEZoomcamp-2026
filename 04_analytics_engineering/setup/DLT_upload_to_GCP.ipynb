{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC2QnhmKxpq1"
   },
   "source": [
    "**Please set up your credentials JSON as GCP_CREDENTIALS secrets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UsUZobVduL7l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage, bigquery\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bq_client = bigquery.Client()\n",
    "bucket = storage_client.bucket(\"dtc-de-zoomcamp-didac\")\n",
    "blob = bucket.blob(\"nytaxi/_manifests/module4_processed_periods.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lYh7r1mTf4uo"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from dlt.destinations import filesystem\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76zT1PzAgs7A"
   },
   "source": [
    "Ingesting parquet files to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "id": "xya0215jsnsb",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading yellow_tripdata_2019-01.csv.gz\n",
      "Downloading yellow_tripdata_2020-01.csv.gz\n",
      "Downloading green_tripdata_2019-01.csv.gz\n",
      "Downloading green_tripdata_2020-01.csv.gz\n",
      "Pipeline module4_github_rides_pipeline load step completed in 5.25 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset nytaxi\n",
      "The filesystem destination used gs://dtc-de-zoomcamp-didac location to store data\n",
      "Load package 1771016474.0163293 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "def load_manifest():\n",
    "    if blob.exists():\n",
    "        return json.loads(blob.download_as_text())\n",
    "    return {\"processed_periods\": []}\n",
    "\n",
    "def save_manifest(manifest: dict):\n",
    "    blob.upload_from_string(\n",
    "        json.dumps(manifest, indent=2),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    "\n",
    "    \n",
    "# Define a dlt source to download and process gz compressed csv files as resources\n",
    "@dlt.source(name=\"module4_github_rides\")\n",
    "def download_gz_csv():\n",
    "    # ✅ state propio (manifest) en GCS\n",
    "    manifest = load_manifest()\n",
    "    processed = set(manifest.get(\"processed_periods\", []))\n",
    "    \n",
    "    colors = [\"yellow\", \"green\"]\n",
    "    years = [2019, 2020]\n",
    "\n",
    "    buffers = {c: [] for c in colors}\n",
    "    to_mark = {c: [] for c in colors}\n",
    "\n",
    "    for color in colors:\n",
    "        for year in years:\n",
    "            for month in range(1,2):\n",
    "\n",
    "                month_str = f\"{month:02d}\"\n",
    "                period_key = f\"{color}_{year}-{month_str}\"\n",
    "                \n",
    "                 # Skip if already processed\n",
    "                if period_key in processed:\n",
    "                    print(f\"Skipping {period_key}\")\n",
    "                    continue\n",
    "                    \n",
    "                file_name = f\"{color}_tripdata_{year}-{month_str}.csv.gz\"\n",
    "                url = f\"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{color}/{file_name}\"\n",
    "\n",
    "                print(f\"Downloading {file_name}\")\n",
    "\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "        \n",
    "                df = pd.read_csv(BytesIO(response.content), compression=\"gzip\", dtype={\"store_and_fwd_flag\": \"string\"})\n",
    "\n",
    "                # Add usefull metadata\n",
    "                df[\"year\"] = year\n",
    "                df[\"month\"] = month\n",
    "                df[\"color\"] = color\n",
    "        \n",
    "                # Save the state\n",
    "                buffers[color].append(df)\n",
    "                to_mark[color].append(period_key)\n",
    "\n",
    "    for color in colors:\n",
    "        if buffers[color]:  # si hay algo\n",
    "            big_df = pd.concat(buffers[color], ignore_index=True)\n",
    "\n",
    "            # ✅ actualizamos manifest SOLO cuando vamos a emitir ese color\n",
    "            processed.update(to_mark[color])\n",
    "            manifest[\"processed_periods\"] = sorted(processed)\n",
    "            save_manifest(manifest)\n",
    "            \n",
    "            yield dlt.resource(big_df, name=f\"{color}_tripdata\")\n",
    "            \n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"module4_github_rides_pipeline\",\n",
    "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
    "    dataset_name=\"nytaxi\",\n",
    ")\n",
    "\n",
    "# Run the pipeline to load Parquet data into DuckDB\n",
    "load_info = pipeline.run(download_gz_csv(), loader_file_format=\"csv\")\n",
    "\n",
    "# Print the results\n",
    "print(load_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"module4_github_rides_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"nytaxi\"\n",
    ")\n",
    "\n",
    "print(pipeline.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "blobs = list(bucket.list_blobs(prefix=\"nytaxi/_dlt_pipeline_state/\"))\n",
    "\n",
    "for b in blobs:\n",
    "    print(\"Reading:\", b.name)\n",
    "    content = b.download_as_text()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_state_version\": 1,\n",
      "  \"_state_engine_version\": 4,\n",
      "  \"_local\": {\n",
      "    \"first_run\": false,\n",
      "    \"initial_cwd\": \"/app\",\n",
      "    \"last_run_context\": {\n",
      "      \"settings_dir\": \"/app/.dlt\",\n",
      "      \"local_dir\": \"/app\",\n",
      "      \"run_dir\": \"/app\",\n",
      "      \"uri\": \"file:///app\"\n",
      "    },\n",
      "    \"_last_extracted_at\": \"2026-02-13 13:26:08.335818+00:00\",\n",
      "    \"_last_extracted_hash\": \"Af3XU4WclFqPkiAQb69xI82XW2SIhH2qSN36lSkvMO0=\"\n",
      "  },\n",
      "  \"schema_names\": [\n",
      "    \"module4_github_rides\"\n",
      "  ],\n",
      "  \"pipeline_name\": \"module4_github_rides_pipeline\",\n",
      "  \"dataset_name\": \"nytaxi\",\n",
      "  \"default_schema_name\": \"module4_github_rides\",\n",
      "  \"destination_type\": \"dlt.destinations.filesystem\",\n",
      "  \"destination_name\": null,\n",
      "  \"_version_hash\": \"Af3XU4WclFqPkiAQb69xI82XW2SIhH2qSN36lSkvMO0=\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dlt.destinations import filesystem\n",
    "import dlt, json\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"module4_github_rides_pipeline\",\n",
    "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
    "    dataset_name=\"nytaxi\",\n",
    ")\n",
    "\n",
    "print(json.dumps(pipeline.state, indent=2, default=str)[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def find_key(obj, key=\"processed_periods\", path=\"\"):\n",
    "    hits = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            p = f\"{path}.{k}\" if path else k\n",
    "            if k == key:\n",
    "                hits.append((p, v))\n",
    "            hits.extend(find_key(v, key, p))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            hits.extend(find_key(v, key, f\"{path}[{i}]\"))\n",
    "    return hits\n",
    "\n",
    "hits = find_key(pipeline.state, \"processed_periods\")\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
